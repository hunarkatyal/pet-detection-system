# -*- coding: utf-8 -*-
"""pet detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KKMJx2F7Cl_1SYAa1QVWrL1JlVxq0v1u

Installing the required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install torch torchvision torchaudio --quiet
!pip install opencv-python-headless --quiet
!pip install pandas matplotlib seaborn --quiet
!pip install twilio --quiet  # For SMS notifications
!pip install pywhatkit --quiet  # For WhatsApp notifications
!git clone https://github.com/ultralytics/yolov5  # Clone YOLOv5 repository
# %cd yolov5
!pip install -r requirements.txt  # Install YOLOv5 dependencies
!pip install moviepy --quiet  # For video processing
# %cd ..

import torch
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import HTML, display
from google.colab import files
from google.colab.patches import cv2_imshow
import time
import os
import sys
import shutil
from moviepy.editor import VideoFileClip
from IPython.display import Image, clear_output
import glob
from pathlib import Path
from datetime import datetime
from twilio.rest import Client  # For SMS

# Try importing pywhatkit with error handling
try:
    import pywhatkit
    print("DEBUG: pywhatkit imported successfully")
    WHATSAPP_AVAILABLE = True
except (ImportError, KeyError) as e:
    print(f"DEBUG: WhatsApp functionality not available. Error: {e}")
    WHATSAPP_AVAILABLE = False

# Add YOLOv5 to path
sys.path.append('yolov5')
print("DEBUG: Added YOLOv5 to path")

# Suppress warnings
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
os.environ['PYTHONWARNINGS'] = 'ignore'
print("DEBUG: Warnings suppressed")

"""Declaration of functions"""

import torch
import cv2
import numpy as np
import pandas as pd
import time
import os
import sys
import glob  # Add this import
from datetime import datetime
import shutil
import matplotlib.pyplot as plt

# Set up WhatsApp functionality check without dependency on display
try:
    import pywhatkit
    WHATSAPP_AVAILABLE = True
    print("WhatsApp functionality available")
except (ImportError, KeyError) as e:
    WHATSAPP_AVAILABLE = False
    print(f"WhatsApp functionality not available. Error: {e}")

def load_yolo_model():
    """Load YOLOv5 model from PyTorch Hub"""
    print("Loading YOLOv5 model...")
    try:
        model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
        print("Model loaded successfully")
        return model
    except Exception as e:
        print(f"Error loading model: {e}")
        print("Trying alternative loading method...")
        # Alternative: Try to load locally if already downloaded
        try:
            if os.path.exists('yolov5'):
                model = torch.hub.load('yolov5', 'custom', path='yolov5s.pt', source='local')
                print("Model loaded successfully from local path")
                return model
            else:
                print("YOLOv5 directory not found. Downloading repository...")
                os.system('git clone https://github.com/ultralytics/yolov5')
                # Install dependencies to ensure it works
                os.system('pip install -r yolov5/requirements.txt')
                model = torch.hub.load('yolov5', 'custom', path='yolov5/yolov5s.pt', source='local')
                print("Model loaded successfully after downloading")
                return model
        except Exception as e2:
            print(f"Failed to load model: {e2}")
            sys.exit("Cannot continue without a working model. Please check your internet connection and try again.")

# [Other functions unchanged...]

def get_video_from_upload_or_path(default_path=None):
    """Get video file from path without using Google Colab functions"""
    print("\nVideo input options:")
    print("1. Enter a video file path")
    if default_path and os.path.exists(default_path):
        print(f"2. Use default path: {default_path}")

    while True:
        choice = input("Enter your choice (1" + (" or 2" if default_path and os.path.exists(default_path) else "") + "): ")

        if choice == '1':
            video_path = input("Enter the full path to your video file: ")
            if os.path.exists(video_path) and os.path.isfile(video_path):
                print(f"Video selected: {video_path}")
                return video_path
            else:
                print("File not found. Please try again.")

        elif choice == '2' and default_path and os.path.exists(default_path):
            print(f"Using default video: {default_path}")
            return default_path

        else:
            print("Invalid choice. Please try again.")

def send_notification(message, notification_settings):
    """Send notification based on the configured notification type"""
    try:
        if notification_settings['type'] == 'sms':
            # Twilio SMS notifications
            try:
                client = Client(
                    notification_settings['twilio_account_sid'],
                    notification_settings['twilio_auth_token']
                )

                client.messages.create(
                    body=message,
                    from_=notification_settings['twilio_number'],
                    to=notification_settings['phone_number']
                )
                print(f"SMS notification sent to {notification_settings['phone_number']}")
                return True
            except Exception as e:
                print(f"Error sending SMS: {e}")
                return False

        elif notification_settings['type'] == 'whatsapp' and WHATSAPP_AVAILABLE:
            # WhatsApp notifications
            try:
                # Format phone number for pywhatkit (remove + and spaces)
                phone = notification_settings['phone_number'].replace('+', '').replace(' ', '')

                # Send WhatsApp message
                # Get current time and add 1 minute
                now = datetime.now()
                hour = now.hour
                minute = now.minute + 1
                if minute >= 60:
                    minute = 0
                    hour += 1
                if hour >= 24:
                    hour = 0

                pywhatkit.sendwhatmsg(phone, message, hour, minute, wait_time=10)
                print(f"WhatsApp notification sent to {notification_settings['phone_number']}")
                return True
            except Exception as e:
                print(f"Error sending WhatsApp message: {e}")
                return False

        return False
    except Exception as e:
        print(f"Error in send_notification: {e}")
        return False

def update_progress_bar(current, total, start_time=None, prefix='Progress:', suffix='Complete', bar_length=50):
    """
    Display a customizable progress bar

    Parameters:
    - current: Current progress value
    - total: Total value for 100% progress
    - start_time: Starting time to calculate ETA (if None, no ETA is shown)
    - prefix: Text displayed before the progress bar
    - suffix: Text displayed after the progress bar
    - bar_length: Length of the progress bar in characters
    """
    try:
        # Get terminal width for dynamic progress bar
        try:
            term_width = os.get_terminal_size().columns
        except (AttributeError, OSError):
            term_width = 80

        # Adjust bar length to fit terminal
        bar_length = min(bar_length, term_width - (len(prefix) + len(suffix) + 20))

        # Calculate progress
        progress = current / total
        filled_length = int(bar_length * progress)
        bar = '█' * filled_length + '-' * (bar_length - filled_length)
        percentage = progress * 100

        # Calculate ETA if start time is provided
        eta_str = ""
        if start_time is not None:
            elapsed_time = time.time() - start_time
            if progress > 0:
                eta = elapsed_time / progress - elapsed_time
                if eta > 60:
                    eta_str = f" ETA: {eta // 60:.0f}m {eta % 60:.0f}s"
                else:
                    eta_str = f" ETA: {eta:.1f}s"
            else:
                eta_str = " ETA: calculating..."

        # Format full progress bar text
        progress_text = f"\r{prefix} [{bar}] {percentage:.1f}% ({current}/{total}){eta_str} {suffix}"

        # Print progress bar
        sys.stdout.write(progress_text)
        sys.stdout.flush()

        # Add newline if complete
        if current >= total:
            if start_time is not None:
                elapsed_time = time.time() - start_time
                if elapsed_time > 60:
                    time_str = f" in {elapsed_time // 60:.0f}m {elapsed_time % 60:.1f}s"
                else:
                    time_str = f" in {elapsed_time:.1f}s"
            else:
                time_str = ""

            sys.stdout.write(f"\r{prefix} [{bar}] 100.0% ({total}/{total}){time_str} {suffix}\n")
            sys.stdout.flush()

    except Exception as e:
        # Failsafe to prevent progress bar errors from affecting main functionality
        print(f"\nProgress bar error: {e}")

def create_pet_movement_heatmap(video_path, pet_positions):
    """Create a heatmap visualization of pet movements"""
    if not pet_positions:
        print("No pet positions recorded for heatmap generation")
        return None

    try:
        print("\nGenerating pet movement heatmap...")

        # Open video to get dimensions
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"Error: Could not open video {video_path}")
            return None

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()

        # Create a blank canvas for the heatmap
        heatmap = np.zeros((height, width), dtype=np.float32)

        # Set up progress tracking for heatmap generation
        start_time = time.time()
        total_positions = len(pet_positions)

        # Add each pet position to the heatmap with Gaussian blur
        for idx, (_, x, y, _) in enumerate(pet_positions):
            if idx % max(1, total_positions // 100) == 0 or idx == total_positions - 1:
                update_progress_bar(idx + 1, total_positions, start_time,
                                   prefix='Generating heatmap:', suffix='')

            if 0 <= x < width and 0 <= y < height:
                # Create a small intensity map around each point
                radius = 20  # Size of the heat area
                y1, y2 = max(0, y-radius), min(height, y+radius)
                x1, x2 = max(0, x-radius), min(width, x+radius)

                for yi in range(y1, y2):
                    for xi in range(x1, x2):
                        # Calculate distance from center point
                        dist = np.sqrt((xi-x)**2 + (yi-y)**2)
                        if dist <= radius:
                            # Add intensity inversely proportional to distance
                            intensity = 1.0 - (dist / radius)
                            heatmap[yi, xi] += intensity

        # Normalize heatmap for visualization
        if np.max(heatmap) > 0:
            heatmap = heatmap / np.max(heatmap)

        # Create RGB heatmap with colormap
        heatmap_color = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)

        # Read first frame as background
        cap = cv2.VideoCapture(video_path)
        ret, background = cap.read()
        cap.release()

        if not ret:
            # If can't read frame, create blank background
            background = np.zeros((height, width, 3), dtype=np.uint8)

        # Blend heatmap with background
        alpha = 0.6  # Transparency factor
        overlay = cv2.addWeighted(background, 1-alpha, heatmap_color, alpha, 0)

        # Add labels and title
        cv2.putText(overlay, "Pet Movement Heatmap", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        # Add timestamp
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(overlay, f"Generated: {timestamp}", (10, height - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # Save the heatmap
        heatmap_path = "pet_movement_heatmap.jpg"
        cv2.imwrite(heatmap_path, overlay)

        print(f"\nHeatmap saved to {heatmap_path}")

        return heatmap_path

    except Exception as e:
        print(f"\nError creating heatmap: {e}")
        import traceback
        traceback.print_exc()
        return None

def process_video(input_path, output_path, model, line_coords=None, frame_skip=2, notification_settings=None):
    """Process video for pet detection and line crossing events"""
    # Initialize variables
    stats = {
        'total_frames': 0,
        'processed_frames': 0,
        'frames_with_pets': 0,
        'frames_with_dogs': 0,
        'frames_with_cats': 0,
        'line_crossing_count': 0,
        'pet_positions': [],
        'avg_detection_time': 0,
        'avg_fps': 0,
        'total_processing_time': 0,
        'processed_ratio': 0,
        'pet_detection_rate': 0
    }

    # Initialize variables for tracking
    detection_times = []
    start_time = time.time()
    last_notification_time = 0
    min_time_between_notifications = 60  # seconds

    # Check if input file exists
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} not found.")
        return None, None

    # Open the video file
    try:
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            print(f"Error: Could not open video {input_path}")
            return None, None

        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        stats['total_frames'] = total_frames

        # Set up output video writer
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # Set up line crossing detection
        if line_coords is None:
            line_coords = ((0, height // 2), (width, height // 2))

        # Pet tracking variables
        pet_trackers = {}
        next_tracker_id = 0
        line_crossed_ids = set()

        # Process frames
        frame_count = 0

        # Define pet classes in COCO dataset
        pet_classes = {
            15: 'cat',  # Index 15 is cat in COCO
            16: 'dog'   # Index 16 is dog in COCO
        }

        # Set up progress bar
        print("\nProcessing video frames:")
        bar_length = 50
        update_interval = max(1, total_frames // 100)  # Update progress bar every 1% of frames

        # Get terminal width for dynamic progress bar (fallback to 80 if can't determine)
        try:
            term_width = os.get_terminal_size().columns
        except (AttributeError, OSError):
            term_width = 80

        bar_length = min(bar_length, term_width - 30)  # Ensure progress bar fits in terminal

        last_update_time = time.time()

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # Update progress bar (only update periodically to avoid slowing down processing)
            if frame_count % update_interval == 0 or frame_count == 1 or frame_count == total_frames:
                progress = frame_count / total_frames
                filled_length = int(bar_length * progress)
                bar = '█' * filled_length + '-' * (bar_length - filled_length)

                # Calculate ETA
                elapsed_time = time.time() - start_time
                if progress > 0:
                    eta = elapsed_time / progress - elapsed_time
                    if eta > 60:
                        eta_str = f"ETA: {eta // 60:.0f}m {eta % 60:.0f}s"
                    else:
                        eta_str = f"ETA: {eta:.1f}s"
                else:
                    eta_str = "ETA: calculating..."

                # Format and print progress bar with frame count and percentage
                sys.stdout.write(f"\r[{bar}] {progress*100:.1f}% ({frame_count}/{total_frames}) {eta_str}")
                sys.stdout.flush()

                last_update_time = time.time()

            # Process based on frame skip setting
            # New logic: if frame_skip is 1, process every frame
            process_current_frame = (frame_count % frame_skip == 0) or (frame_skip == 1)

            if not process_current_frame:
                out.write(frame)  # Write frame without processing
                continue

            stats['processed_frames'] += 1

            # Detect pets in frame
            t_start = time.time()
            results = model(frame)
            t_end = time.time()
            detection_times.append(t_end - t_start)

            # Get detections
            detections = results.pandas().xyxy[0]
            pets_detected = False
            cats_detected = False
            dogs_detected = False
            pet_boxes = []

            # Draw monitoring line
            cv2.line(frame, line_coords[0], line_coords[1], (0, 255, 0), 2)

            # Process detections
            for _, det in detections.iterrows():
                if int(det['class']) in pet_classes:
                    pets_detected = True
                    x1, y1, x2, y2 = int(det['xmin']), int(det['ymin']), int(det['xmax']), int(det['ymax'])
                    conf = det['confidence']
                    cls = int(det['class'])
                    cls_name = pet_classes[cls]

                    # Update stats
                    if cls_name == 'cat':
                        cats_detected = True
                    elif cls_name == 'dog':
                        dogs_detected = True

                    # Store position for heatmap
                    center_x = (x1 + x2) // 2
                    center_y = (y1 + y2) // 2
                    stats['pet_positions'].append((frame_count, center_x, center_y, cls_name))

                    # Check for line crossing
                    color = (0, 165, 255)  # Default: orange
                    label = f"{cls_name} {conf:.2f}"

                    # Draw bounding box
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                    # Draw label
                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                    # Check if pet crossed the line
                    bottom_center = ((x1 + x2) // 2, y2)
                    pet_boxes.append((bottom_center, cls_name))

            # Check for line crossings
            for (point, pet_type) in pet_boxes:
                # Basic line crossing detection
                line_start, line_end = line_coords

                # Check if point crosses line (simplified)
                d1 = (point[0] - line_start[0]) * (line_end[1] - line_start[1]) - \
                     (point[1] - line_start[1]) * (line_end[0] - line_start[0])

                # If d1 is near zero, the point is close to the line
                if abs(d1) < 1000:  # Threshold can be adjusted
                    current_time = time.time()

                    # Throttle notifications to prevent spam
                    if current_time - last_notification_time > min_time_between_notifications:
                        stats['line_crossing_count'] += 1

                        # Send notification if enabled
                        if notification_settings and notification_settings['enabled']:
                            send_notification(
                                f"Pet Alert: {pet_type} detected crossing your monitoring line!",
                                notification_settings
                            )
                            last_notification_time = current_time

            # Update stats
            if pets_detected:
                stats['frames_with_pets'] += 1
            if cats_detected:
                stats['frames_with_cats'] += 1
            if dogs_detected:
                stats['frames_with_dogs'] += 1

            # Add frame counter
            cv2.putText(frame, f"Frame: {frame_count}/{total_frames}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

            # Write the processed frame
            out.write(frame)

        # Complete the progress bar
        progress = 1.0
        filled_length = int(bar_length * progress)
        bar = '█' * filled_length + '-' * (bar_length - filled_length)
        elapsed_time = time.time() - start_time

        if elapsed_time > 60:
            time_str = f" Complete in {elapsed_time // 60:.0f}m {elapsed_time % 60:.1f}s"
        else:
            time_str = f" Complete in {elapsed_time:.1f}s"

        sys.stdout.write(f"\r[{bar}] 100.0% ({total_frames}/{total_frames}){time_str}\n")
        sys.stdout.flush()

        # Calculate stats
        stats['avg_detection_time'] = sum(detection_times) / len(detection_times) if detection_times else 0
        stats['avg_fps'] = 1.0 / stats['avg_detection_time'] if stats['avg_detection_time'] > 0 else 0
        stats['total_processing_time'] = elapsed_time
        stats['processed_ratio'] = stats['processed_frames'] / stats['total_frames'] if stats['total_frames'] > 0 else 0
        stats['pet_detection_rate'] = stats['frames_with_pets'] / stats['processed_frames'] if stats['processed_frames'] > 0 else 0

        # Clean up
        cap.release()
        out.release()

        return output_path, stats

    except Exception as e:
        print(f"\nError during video processing: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def run_pet_detection():
    """Main function to run pet detection on uploaded video with notification capabilities"""
    print("Pet Detection System using YOLOv5")
    print("=================================")

    # Load YOLOv5 model
    model = load_yolo_model()
    if model is None:
        print("Failed to load model. Exiting.")
        return

    # Get video file
    print("\nSelect video file:")
    # Check for existing video files in current directory for convenience
    video_files = []
    for ext in ['.mp4', '.avi', '.mov', '.mkv']:
        video_files.extend(glob.glob(f'*{ext}'))

    default_path = None
    if video_files:
        print("Found these video files in current directory:")
        for i, file in enumerate(video_files, 1):
            print(f"{i}. {file}")
        try:
            idx = int(input(f"Select a video (1-{len(video_files)}) or press Enter to specify path: ")) - 1
            if 0 <= idx < len(video_files):
                default_path = video_files[idx]
        except (ValueError, IndexError):
            pass

    video_file = get_video_from_upload_or_path(default_path)

    if not video_file:
        print("No video file selected. Please run again and select a video file.")
        return

    input_path = video_file
    output_path = "pet_detection_output.mp4"

    # Processing speed configuration
    print("\nProcessing speed configuration:")
    print("1. Process every frame (highest accuracy, slowest)")
    print("2. Skip frames (faster processing)")

    process_choice = input("Enter your choice (1-2, default=2): ")

    if process_choice == '1':
        # Process all frames (no skipping)
        frame_skip = 1
        print("Processing every frame for maximum accuracy")
    else:
        # Process with frame skipping for speed
        print("Frame skip configuration:")
        print("Higher values = faster processing but may miss some detections")
        try:
            frame_skip_input = input("Enter frame skip value (2-10, default=2): ")
            frame_skip = int(frame_skip_input) if frame_skip_input else 2
            if frame_skip < 2:
                print("Invalid value. Using default value of 2.")
                frame_skip = 2
            elif frame_skip > 10:
                print("Warning: High frame skip values may miss important events.")
        except ValueError:
            print("Invalid input. Using default value of 2.")
            frame_skip = 2

    # Get video dimensions for line crossing
    try:
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            print(f"Error: Could not open video {input_path}")
            return
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()
    except Exception as e:
        print(f"Error getting video dimensions: {e}")
        return

    # Get line crossing configuration
    print("\nLine crossing detection setup:")
    use_custom_line = input("Would you like to set a custom line for crossing detection? (y/n): ").lower() == 'y'

    if use_custom_line:
        print(f"Video dimensions: Width={width}, Height={height}")
        print("Enter line coordinates (x1,y1,x2,y2) as percentages of video width/height (0-100)")
        try:
            x1_pct = float(input("Start X position (%): "))
            y1_pct = float(input("Start Y position (%): "))
            x2_pct = float(input("End X position (%): "))
            y2_pct = float(input("End Y position (%): "))

            # Convert percentages to actual coordinates
            x1 = int(width * x1_pct / 100)
            y1 = int(height * y1_pct / 100)
            x2 = int(width * x2_pct / 100)
            y2 = int(height * y2_pct / 100)

            line_coords = ((x1, y1), (x2, y2))
        except ValueError:
            print("Invalid input. Using default horizontal line.")
            line_coords = ((0, height // 2), (width, height // 2))
    else:
        # Use default line (horizontal line in the middle)
        line_coords = ((0, height // 2), (width, height // 2))
        print(f"Using default horizontal line at y={height//2}")

    # Notification setup - modified to include hardcoded Twilio credentials
    print("\nNotification setup:")
    enable_notifications = input("Would you like to receive notifications when pets cross the line? (y/n): ").lower() == 'y'

    notification_settings = None
    if enable_notifications:
        notification_type = input("Choose notification type (sms/whatsapp): ").lower()
        if notification_type not in ['sms', 'whatsapp']:
            print("Invalid notification type. Using SMS as default.")
            notification_type = 'sms'

        if notification_type == 'whatsapp' and not WHATSAPP_AVAILABLE:
            print("WhatsApp functionality not available. Using SMS instead.")
            notification_type = 'sms'

        phone_number = input("Enter your phone number with country code (e.g., +1234567890): ")

        notification_settings = {
            'enabled': True,
            'type': notification_type,
            'phone_number': phone_number
        }

        # Add hardcoded Twilio credentials if using SMS
        if notification_type == 'sms':
            # Hardcoded Twilio credentials
            notification_settings['twilio_account_sid'] = ''
            notification_settings['twilio_auth_token'] = ''
            notification_settings['twilio_number'] = ''
            print("\nTwilio credentials have been configured automatically")

        print(f"\nNotifications will be sent to {phone_number} via {notification_type.upper()}")
    else:
        print("Notifications disabled.")

    # Show processing mode summary
    if frame_skip == 1:
        print(f"\nProcessing video: {video_file}")
        print("Processing mode: Every frame (maximum accuracy)")
    else:
        print(f"\nProcessing video: {video_file}")
        print(f"Processing mode: Analyzing 1 frame for every {frame_skip} frames")

    print("This may take a while depending on the video length...")

    # Process the video
    try:
        # Call to your process_video function here
        result_path, stats = process_video(
            input_path, output_path, model,
            line_coords=line_coords,
            frame_skip=frame_skip,
            notification_settings=notification_settings
        )

        if not result_path or not stats:
            print("Error processing video")
            return

    except Exception as e:
        print(f"Error during video processing: {e}")
        return

    # Create movement heatmap
    print("\nCreating pet movement heatmap...")
    try:
        heatmap_path = create_pet_movement_heatmap(input_path, stats['pet_positions'])
    except Exception as e:
        print(f"Error creating heatmap: {e}")
        heatmap_path = None

    # Display results
    print("\nProcessing complete. Results:")
    print(f"• Total frames in video: {stats['total_frames']}")
    print(f"• Frames processed: {stats['processed_frames']} ({stats['processed_ratio']:.1%} of video)")
    print(f"• Frames with pets: {stats['frames_with_pets']} ({stats['pet_detection_rate']:.1%} of processed frames)")
    print(f"• Frames with dogs: {stats['frames_with_dogs']}")
    print(f"• Frames with cats: {stats['frames_with_cats']}")
    print(f"• Average processing time: {stats['avg_detection_time']:.4f} seconds per frame")
    print(f"• Average processing FPS: {stats['avg_fps']:.2f}")
    print(f"• Total processing time: {stats['total_processing_time']:.2f} seconds")
    print(f"• Line crossing count: {stats['line_crossing_count']}")

    if notification_settings and notification_settings['enabled']:
        print(f"• Notifications sent: {stats['line_crossing_count']} ({notification_settings['type']})")

    print(f"\nProcessed video saved to: {output_path}")
    if heatmap_path:
        print(f"Pet movement heatmap saved to: {heatmap_path}")

    print("\nPet detection process complete!")

# Run the pet detection system
if __name__ == "__main__":
    run_pet_detection()

from google.colab import drive
drive.mount('/content/drive')

"""Execution"""
